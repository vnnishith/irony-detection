{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANLP_18231377_Assignment1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qndnYAQpL6I8",
        "colab_type": "text"
      },
      "source": [
        "###  Irony detection in tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEvckziyL6I9",
        "colab_type": "text"
      },
      "source": [
        "```csv\n",
        "Tweet index     Label   Tweet text\n",
        "1       1       Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
        "2       1       @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\n",
        "3       1       Hey there! Nice to see you Minnesota/ND Winter Weather \n",
        "4       0       3 episodes left I'm dying over here\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsp4qoMk2dqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from array import array\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, TimeDistributed, Dense, Bidirectional, Dropout\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybREvGbSF8av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1cL4ZfUC666",
        "colab_type": "code",
        "outputId": "23420c65-4866-46b0-83e1-5572ac7cb851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFh4RlpZS7oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'SemEval2018-T3-train-taskA.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQVm3ist2eet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = []\n",
        "#All the tweets, index and labels are read from the file and stored in a dataframe\n",
        "def load_tweets(preserve_case = False):\n",
        "    file =open(filepath, encoding=\"UTF-8\")\n",
        "    lines=  file.readlines()\n",
        "    del lines[0]\n",
        "    dict = {\"Tweet index\": [], \"Label\": [], \"Tweet text\": []}\n",
        "    for line in lines:\n",
        "      values = line.split('\\t')\n",
        "      label = int(values[1])\n",
        "      dict[\"Tweet index\"].append(values[0])\n",
        "      dict[\"Label\"].append(label)\n",
        "      tweet = str.lower(values[2]) if preserve_case is False else values[2]\n",
        "      dict[\"Tweet text\"].append(tweet)\n",
        "      values[0] = int(values[0])\n",
        "      values[1] = label\n",
        "      values[2] = word_tokenize(tweet)\n",
        "      all_data.append(tuple(values))\n",
        "    return pd.DataFrame(dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-XZLWTsUy",
        "colab_type": "code",
        "outputId": "71d039e5-86cc-4266-cb19-476d5fb5f96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "formatted_data = load_tweets()\n",
        "formatted_data[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet index</th>\n",
              "      <th>Tweet text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>sweet united nations video. just in time for c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>@mrdahl87 we are rumored to have talked to erv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>hey there! nice to see you minnesota/nd winter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3 episodes left i'm dying over here\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>\"i can't breathe!\" was chosen as the most nota...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label Tweet index                                         Tweet text\n",
              "0      1           1  sweet united nations video. just in time for c...\n",
              "1      1           2  @mrdahl87 we are rumored to have talked to erv...\n",
              "2      1           3  hey there! nice to see you minnesota/nd winter...\n",
              "3      0           4              3 episodes left i'm dying over here\\n\n",
              "4      1           5  \"i can't breathe!\" was chosen as the most nota..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hO03savLJV6",
        "colab_type": "code",
        "outputId": "2a01a00a-59b3-4cf6-ce06-e42b498e6ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "source": [
        "# format used to train the model in task 4\n",
        "all_data[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1,\n",
              "  1,\n",
              "  ['sweet',\n",
              "   'united',\n",
              "   'nations',\n",
              "   'video',\n",
              "   '.',\n",
              "   'just',\n",
              "   'in',\n",
              "   'time',\n",
              "   'for',\n",
              "   'christmas',\n",
              "   '.',\n",
              "   '#',\n",
              "   'imagine',\n",
              "   '#',\n",
              "   'noreligion',\n",
              "   'http',\n",
              "   ':',\n",
              "   '//t.co/fej2v3oubr']),\n",
              " (2,\n",
              "  1,\n",
              "  ['@',\n",
              "   'mrdahl87',\n",
              "   'we',\n",
              "   'are',\n",
              "   'rumored',\n",
              "   'to',\n",
              "   'have',\n",
              "   'talked',\n",
              "   'to',\n",
              "   'erv',\n",
              "   \"'s\",\n",
              "   'agent',\n",
              "   '...',\n",
              "   'and',\n",
              "   'the',\n",
              "   'angels',\n",
              "   'asked',\n",
              "   'about',\n",
              "   'ed',\n",
              "   'escobar',\n",
              "   '...',\n",
              "   'that',\n",
              "   \"'s\",\n",
              "   'hardly',\n",
              "   'nothing',\n",
              "   ';',\n",
              "   ')']),\n",
              " (3,\n",
              "  1,\n",
              "  ['hey',\n",
              "   'there',\n",
              "   '!',\n",
              "   'nice',\n",
              "   'to',\n",
              "   'see',\n",
              "   'you',\n",
              "   'minnesota/nd',\n",
              "   'winter',\n",
              "   'weather'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMLocSAg2esW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ironic_tweets = formatted_data.loc[formatted_data['Label'] == 1]\n",
        "not_ironic_tweets = formatted_data.loc[formatted_data['Label'] == 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l33sKp4i2rFK",
        "colab_type": "code",
        "outputId": "98e3f2cf-d849-4867-8fb6-a572dfc5e374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(not_ironic_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMO5LUnn20tn",
        "colab_type": "code",
        "outputId": "43c3dd29-165d-4408-ec5a-51aec77ffde1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(ironic_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1911"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFfoHlVX22_7",
        "colab_type": "code",
        "outputId": "3c05fc12-fc5b-48ba-d335-cf07afbe4dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_tweets = formatted_data['Tweet text'].str.cat(sep=' ')\n",
        "tokenized_words = word_tokenize(all_tweets)\n",
        "total_vocabulary = set(tokenized_words)\n",
        "len(total_vocabulary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13443"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESlLwnmuIlt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words():\n",
        "  vectorizer = CountVectorizer()\n",
        "  X = vectorizer.fit_transform(formatted_data['Tweet text'])\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdwv-qzDI9Br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words_alternate():\n",
        "  X = []\n",
        "  for sentence in formatted_data['Tweet text']:\n",
        "    x_sent = []\n",
        "    sent_words = word_tokenize(sentence.lower())\n",
        "    frequencies_words = Counter(sent_words)\n",
        "    for voc in vocabulary:\n",
        "      if voc in sent_words:\n",
        "        x_sent.append(frequencies_words[voc])\n",
        "      else:\n",
        "        x_sent.append(0)\n",
        "      X.append(x_sent)\n",
        "  return X      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLMa5AcMJW_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bad_of_words_using_tf_df():\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True).tokenize\n",
        "  vectorizer = TfidfVectorizer(strip_accents=\"unicode\", analyzer=\"word\", tokenizer=tokenizer)\n",
        "  return vectorizer.fit_transform(formatted_data['Tweet text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYCg4pwlivlM",
        "colab_type": "text"
      },
      "source": [
        "##Naive Bayes Classifier\n",
        "Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature\n",
        "\n",
        "It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
        "\n",
        "When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmptVSxtJ0jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NaiveBayes(object):\n",
        "\n",
        "    '''\n",
        "    Naive Bayes classifier - \n",
        "    Reference used: Lecture notes CT475 machine learning and data mining\n",
        "    '''\n",
        "    def train(self, train_data):\n",
        "        '''\n",
        "        input: train_data (data frame containing column Index and Label)\n",
        "        This method is used to calculate the prior probabilites and also voabulary count of tweets for each sentiment\n",
        "        '''\n",
        "        \n",
        "        self.data = train_data\n",
        "        ironic_train, non_ironic_train = self.separate_tweets()\n",
        "        tweet_string = self.data['Tweet text'].str.cat(sep=' ')\n",
        "        tokenized_words = word_tokenize(tweet_string.lower())\n",
        "        self.vocabulary_length = len(set(tokenized_words))\n",
        "        self.ironic_prior_probability = (len(ironic_train)+1)/(len(ironic_train)+len(non_ironic_train)+2)\n",
        "        self.non_ironic_prior_probability = (len(non_ironic_train)+1)/(len(ironic_train)+len(non_ironic_train)+2)\n",
        "        self.ironic_frequency = self.bag_of_words(ironic_train)\n",
        "        self.non_ironic_frequency = self.bag_of_words(non_ironic_train)\n",
        "        self.ironic_vocab_count = self.ironic_frequency.values.sum()\n",
        "        self.non_ironic_vocab_count = self.non_ironic_frequency.values.sum()\n",
        "                \n",
        "    def separate_tweets(self):\n",
        "        ironic_tweets = self.data.loc[self.data['Label'] == 1]\n",
        "        not_ironic_tweets = self.data.loc[self.data['Label'] == 0]\n",
        "        return ironic_tweets,not_ironic_tweets\n",
        "\n",
        "    def bag_of_words(self, data):\n",
        "        '''\n",
        "        input: data [same format as the train data]\n",
        "        this method is used to compute the frequency distribution of the tokens  in the tweets\n",
        "        '''\n",
        "        vectorizer = CountVectorizer()\n",
        "        X_array = vectorizer.fit_transform(data['Tweet text']).toarray()\n",
        "        frequency_matrix = pd.DataFrame(X_array,columns = vectorizer.get_feature_names())\n",
        "        return frequency_matrix\n",
        "    \n",
        "    def predict(self, test_data):\n",
        "        '''\n",
        "        input: test data (list of tweets [not tokenized] which have to be classified)\n",
        "        This method is used to calculate the labels based on the training data that was provided to the model\n",
        "        '''\n",
        "        y = []\n",
        "        for sentence in test_data:\n",
        "            ironic_prob = self.ironic_prior_probability\n",
        "            non_ironic_prob = self.non_ironic_prior_probability\n",
        "            for word in word_tokenize(sentence):\n",
        "                word_ironic_freq = self.ironic_frequency[word].sum() +1 if word in self.ironic_frequency else 1            \n",
        "                word_non_ironic_freq = self.non_ironic_frequency[word].sum()+ 1 if word in self.non_ironic_frequency else 1\n",
        "                word_ironic_prob = word_ironic_freq/(self.ironic_vocab_count + self.vocabulary_length)\n",
        "                word_non_ironic_prob = word_non_ironic_freq/(self.non_ironic_vocab_count + self.vocabulary_length)\n",
        "                ironic_prob = ironic_prob * word_ironic_prob\n",
        "                non_ironic_prob = non_ironic_prob * word_non_ironic_prob\n",
        "                ironic_prob = ironic_prob/(ironic_prob+ non_ironic_prob)\n",
        "                non_ironic_prob = 1- ironic_prob\n",
        "                prediction = 1 if (ironic_prob > non_ironic_prob) else 0\n",
        "            y.append(prediction)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXeZcHUNPjhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsQWlTbcT3Ke",
        "colab_type": "text"
      },
      "source": [
        "## Test-Train Split\n",
        "\n",
        "There are 3834 tweets in the input file provided. Using the first 2500 tweets which is close to 65% of the given data as training data and the remaining data to be used as the hold out set or the test data. If there is more training data that is provided to the model ,the better results can be expected .However more training data could out also lead to overfitting. Looking at the first 2500 tweets in the sample the distribution of the positive and the negative labelled tweets seems to be nearly equal. (Distribution is not skewed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJyaCKORjOJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check the length of ironic tweeets in the train dataset\n",
        "ironic_tweets = formatted_data[0:2500].loc[formatted_data['Label'] == 1]\n",
        "not_ironic_tweets = formatted_data[0:2500].loc[formatted_data['Label'] == 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgGjRpwsjS2Q",
        "colab_type": "code",
        "outputId": "fc896a6b-7e16-45af-a0ef-0f53c6f50e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(ironic_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KF2EyDrG_k-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def task3():\n",
        "  return all_data[0:2500],all_data[2500:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-aZUiGN-XtuV"
      },
      "source": [
        "## Evaluation Metric\n",
        "\n",
        "### F1 score \n",
        "F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. If you want to know if your predictions are good, you need these two measures. You can have a precision of 1 (so when you say it's positive, it's actutally positive) but still have a very low recall (you predicted 3 good positives but forgot 15 others). Or you can have a good recall and a bad precision.\n",
        "\n",
        "F1 Score = 2**(Recall) *** Precision) / (Recall + Precision)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpDRgJOIhEpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = NaiveBayes()\n",
        "model.train(formatted_data[0:2500])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8CJ01WOiyW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data, test_labels = formatted_data[2500:]['Tweet text'], formatted_data[2500:]['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kexo46xvhTMr",
        "colab_type": "code",
        "outputId": "9312ead6-d1ba-4e4d-af6d-4be9246a8732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = model.predict(test_data)\n",
        "print(len(y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOA5PZd4iY4p",
        "colab_type": "code",
        "outputId": "9d66db0f-7307-4eae-c162-359a04ac3193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score = f1_score(test_labels, y_pred, average='micro')\n",
        "print(f1_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6221889055472264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FlgQEXg1yo8",
        "colab_type": "code",
        "outputId": "2268a8e8-724b-4773-928e-a376d5688861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(test_labels,y_pred)\n",
        "print(accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6221889055472264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WSQJRtwaejR",
        "colab_type": "code",
        "outputId": "b0dca908-e17b-4736-90f5-47730fa3364f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(test_labels, y_pred,labels=[0, 1])\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[340 336]\n",
            " [168 490]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82JnhmgBL6JA",
        "colab_type": "text"
      },
      "source": [
        "# Task 4 (20 Marks)\n",
        "\n",
        "Run the following code to generate a model from your training set. The training set should be in a variable  called `train` and is assumed to be of the form:\n",
        "\n",
        "```\n",
        "[(1, 1, ['sweet', 'united', 'nations', 'video', '.', 'just', 'in', 'time', 'for', 'christmas', '.', '#', 'imagine', '#', 'noreligion', 'http', ':', '//t.co/fej2v3oubr']), \n",
        " (2, 1, ['@', 'mrdahl87', 'we', 'are', 'rumored', 'to', 'have', 'talked', 'to', 'erv', \"'s\", 'agent', '...', 'and', 'the', 'angels', 'asked', 'about', 'ed', 'escobar', '...', 'that', \"'s\", 'hardly', 'nothing', ';', ')']), \n",
        " (3, 1, ['hey', 'there', '!', 'nice', 'to', 'see', 'you', 'minnesota/nd', 'winter', 'weather']), \n",
        " (4, 0, ['3', 'episodes', 'left', 'i', \"'m\", 'dying', 'over', 'here']), \n",
        " ...\n",
        "]\n",
        " ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZWIVAxlNXZy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQW8N6hJNXXY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1h_5EslIopG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV207juZL6JB",
        "colab_type": "code",
        "outputId": "a043bc4e-34b8-4f86-9c66-06c4f5855158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2808
        }
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.regularizers import L1L2\n",
        "import numpy as np\n",
        "\n",
        "## These values should be set from Task 3\n",
        "train, test = task3()\n",
        "\n",
        "def make_dictionary(train, test):\n",
        "    dictionary = {}\n",
        "    for d in train+test:\n",
        "        for w in d[2]:\n",
        "            if w not in dictionary:\n",
        "                dictionary[w] = len(dictionary)\n",
        "    return dictionary\n",
        "\n",
        "class KerasBatchGenerator(object):\n",
        "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
        "        self.data = data\n",
        "        self.num_steps = num_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.vocabulary = vocabulary\n",
        "        self.current_idx = 0\n",
        "        self.current_sent = 0\n",
        "        self.skip_step = skip_step\n",
        "\n",
        "    def generate(self):\n",
        "        x = np.zeros((self.batch_size, self.num_steps))\n",
        "        y = np.zeros((self.batch_size, self.num_steps, 2))\n",
        "        while True:\n",
        "            for i in range(self.batch_size):\n",
        "                # Choose a sentence and position with at lest num_steps more words\n",
        "                while self.current_idx + self.num_steps >= len(self.data[self.current_sent][2]):\n",
        "                    self.current_idx = self.current_idx % len(self.data[self.current_sent][2])\n",
        "                    self.current_sent += 1\n",
        "                    if self.current_sent >= len(self.data):\n",
        "                        self.current_sent = 0\n",
        "                # The rows of x are set to values like [1,2,3,4,5]\n",
        "                x[i, :] = [self.vocabulary[w] for w in self.data[self.current_sent][2][self.current_idx:self.current_idx + self.num_steps]]\n",
        "                # The rows of y are set to values like [[1,0],[1,0],[1,0],[1,0],[1,0]]\n",
        "                \n",
        "                y[i, :, :] = [[self.data[self.current_sent][1], 1-self.data[self.current_sent][1]]] * self.num_steps\n",
        "                self.current_idx += self.skip_step\n",
        "            yield x, y\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "# Hyperparameters for model\n",
        "vocabulary = make_dictionary(train, test)\n",
        "num_steps = 5\n",
        "batch_size = 25\n",
        "num_epochs = 75 # Reduce this if the model is taking too long to train (or increase for performance)\n",
        "hidden_size = 125 # Increase this to improve perfomance (or increase for performance)\n",
        "use_dropout=True\n",
        "\n",
        "# Create batches for RNN\n",
        "train_data_generator = KerasBatchGenerator(train, num_steps, batch_size, vocabulary,\n",
        "                                           skip_step=num_steps)\n",
        "valid_data_generator = KerasBatchGenerator(test, num_steps, batch_size, vocabulary,\n",
        "                                           skip_step=num_steps)\n",
        "\n",
        "# A double stacked LSTM with dropout and n hidden layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary), hidden_size, input_length=num_steps))\n",
        "model.add(LSTM(hidden_size, return_sequences=True))\n",
        "model.add (LSTM (hidden_size , bias_regularizer=L1L2(l1=0.01, l2=0.01), return_sequences = True ))\n",
        "#model.add(LSTM(hidden_size, return_sequences=True))\n",
        "if use_dropout:\n",
        "    model.add(Dropout(0.3))\n",
        "model.add(TimeDistributed(Dense(2)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Set optimizer and build model\n",
        "optimizer = Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit_generator(train_data_generator.generate(), len(train)//(batch_size*num_steps), num_epochs,\n",
        "                        validation_data=valid_data_generator.generate(),\n",
        "                        validation_steps=len(test)//(batch_size*num_steps))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"final_model.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/75\n",
            "20/20 [==============================] - 3s 162ms/step - loss: 3.1595 - categorical_accuracy: 0.5176 - val_loss: 3.1179 - val_categorical_accuracy: 0.5224\n",
            "Epoch 2/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 3.0667 - categorical_accuracy: 0.5648 - val_loss: 3.0534 - val_categorical_accuracy: 0.5120\n",
            "Epoch 3/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 3.0069 - categorical_accuracy: 0.5096 - val_loss: 2.9573 - val_categorical_accuracy: 0.5536\n",
            "Epoch 4/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 2.9299 - categorical_accuracy: 0.5416 - val_loss: 2.9029 - val_categorical_accuracy: 0.5296\n",
            "Epoch 5/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 2.8636 - categorical_accuracy: 0.5456 - val_loss: 2.8277 - val_categorical_accuracy: 0.5408\n",
            "Epoch 6/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 2.7893 - categorical_accuracy: 0.5856 - val_loss: 2.7525 - val_categorical_accuracy: 0.5328\n",
            "Epoch 7/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 2.6627 - categorical_accuracy: 0.6772 - val_loss: 2.6831 - val_categorical_accuracy: 0.6088\n",
            "Epoch 8/75\n",
            "20/20 [==============================] - 1s 44ms/step - loss: 2.5520 - categorical_accuracy: 0.7264 - val_loss: 2.6992 - val_categorical_accuracy: 0.5176\n",
            "Epoch 9/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 2.4916 - categorical_accuracy: 0.7148 - val_loss: 2.5810 - val_categorical_accuracy: 0.5616\n",
            "Epoch 10/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 2.4331 - categorical_accuracy: 0.7152 - val_loss: 2.5949 - val_categorical_accuracy: 0.5304\n",
            "Epoch 11/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 2.3737 - categorical_accuracy: 0.6948 - val_loss: 2.5392 - val_categorical_accuracy: 0.5408\n",
            "Epoch 12/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 2.2774 - categorical_accuracy: 0.7284 - val_loss: 2.5632 - val_categorical_accuracy: 0.4920\n",
            "Epoch 13/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 2.1617 - categorical_accuracy: 0.7580 - val_loss: 2.3848 - val_categorical_accuracy: 0.5816\n",
            "Epoch 14/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 2.0547 - categorical_accuracy: 0.8000 - val_loss: 2.4722 - val_categorical_accuracy: 0.5368\n",
            "Epoch 15/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.9737 - categorical_accuracy: 0.7952 - val_loss: 2.4111 - val_categorical_accuracy: 0.5184\n",
            "Epoch 16/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.9122 - categorical_accuracy: 0.8000 - val_loss: 2.3569 - val_categorical_accuracy: 0.5624\n",
            "Epoch 17/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.8650 - categorical_accuracy: 0.8040 - val_loss: 2.3155 - val_categorical_accuracy: 0.5592\n",
            "Epoch 18/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.7830 - categorical_accuracy: 0.8120 - val_loss: 2.5022 - val_categorical_accuracy: 0.4968\n",
            "Epoch 19/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.7010 - categorical_accuracy: 0.8284 - val_loss: 2.1690 - val_categorical_accuracy: 0.5688\n",
            "Epoch 20/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.7210 - categorical_accuracy: 0.8032 - val_loss: 2.1051 - val_categorical_accuracy: 0.5496\n",
            "Epoch 21/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 1.6048 - categorical_accuracy: 0.8140 - val_loss: 2.1773 - val_categorical_accuracy: 0.5176\n",
            "Epoch 22/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.5528 - categorical_accuracy: 0.8212 - val_loss: 2.1708 - val_categorical_accuracy: 0.5112\n",
            "Epoch 23/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.4938 - categorical_accuracy: 0.8208 - val_loss: 2.0996 - val_categorical_accuracy: 0.5328\n",
            "Epoch 24/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.4273 - categorical_accuracy: 0.8332 - val_loss: 2.2568 - val_categorical_accuracy: 0.4984\n",
            "Epoch 25/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 1.3806 - categorical_accuracy: 0.8392 - val_loss: 2.0032 - val_categorical_accuracy: 0.5528\n",
            "Epoch 26/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.3480 - categorical_accuracy: 0.8324 - val_loss: 2.0221 - val_categorical_accuracy: 0.5504\n",
            "Epoch 27/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 1.3179 - categorical_accuracy: 0.8216 - val_loss: 1.8993 - val_categorical_accuracy: 0.5232\n",
            "Epoch 28/75\n",
            "20/20 [==============================] - 1s 47ms/step - loss: 1.2553 - categorical_accuracy: 0.8336 - val_loss: 1.9836 - val_categorical_accuracy: 0.5272\n",
            "Epoch 29/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 1.2157 - categorical_accuracy: 0.8288 - val_loss: 1.8282 - val_categorical_accuracy: 0.5248\n",
            "Epoch 30/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 1.1722 - categorical_accuracy: 0.8376 - val_loss: 1.8863 - val_categorical_accuracy: 0.5360\n",
            "Epoch 31/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 1.1169 - categorical_accuracy: 0.8416 - val_loss: 1.8292 - val_categorical_accuracy: 0.5368\n",
            "Epoch 32/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 1.0714 - categorical_accuracy: 0.8412 - val_loss: 1.8485 - val_categorical_accuracy: 0.5592\n",
            "Epoch 33/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.0456 - categorical_accuracy: 0.8384 - val_loss: 1.7205 - val_categorical_accuracy: 0.5288\n",
            "Epoch 34/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 1.0834 - categorical_accuracy: 0.8144 - val_loss: 1.5381 - val_categorical_accuracy: 0.5376\n",
            "Epoch 35/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.9747 - categorical_accuracy: 0.8440 - val_loss: 1.7539 - val_categorical_accuracy: 0.5384\n",
            "Epoch 36/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.9255 - categorical_accuracy: 0.8536 - val_loss: 1.7838 - val_categorical_accuracy: 0.5384\n",
            "Epoch 37/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.8638 - categorical_accuracy: 0.8660 - val_loss: 1.7186 - val_categorical_accuracy: 0.5096\n",
            "Epoch 38/75\n",
            "20/20 [==============================] - 1s 47ms/step - loss: 0.8694 - categorical_accuracy: 0.8372 - val_loss: 1.6299 - val_categorical_accuracy: 0.5792\n",
            "Epoch 39/75\n",
            "20/20 [==============================] - 1s 47ms/step - loss: 0.8393 - categorical_accuracy: 0.8356 - val_loss: 1.6042 - val_categorical_accuracy: 0.5144\n",
            "Epoch 40/75\n",
            "20/20 [==============================] - 1s 54ms/step - loss: 0.7962 - categorical_accuracy: 0.8484 - val_loss: 1.4806 - val_categorical_accuracy: 0.5304\n",
            "Epoch 41/75\n",
            "20/20 [==============================] - 1s 55ms/step - loss: 0.7858 - categorical_accuracy: 0.8428 - val_loss: 1.5304 - val_categorical_accuracy: 0.5448\n",
            "Epoch 42/75\n",
            "20/20 [==============================] - 1s 55ms/step - loss: 0.7021 - categorical_accuracy: 0.8628 - val_loss: 1.7802 - val_categorical_accuracy: 0.5304\n",
            "Epoch 43/75\n",
            "20/20 [==============================] - 1s 55ms/step - loss: 0.6774 - categorical_accuracy: 0.8668 - val_loss: 1.6967 - val_categorical_accuracy: 0.4640\n",
            "Epoch 44/75\n",
            "20/20 [==============================] - 1s 53ms/step - loss: 0.6696 - categorical_accuracy: 0.8556 - val_loss: 1.5566 - val_categorical_accuracy: 0.6032\n",
            "Epoch 45/75\n",
            "20/20 [==============================] - 1s 47ms/step - loss: 0.6364 - categorical_accuracy: 0.8544 - val_loss: 1.5168 - val_categorical_accuracy: 0.5416\n",
            "Epoch 46/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.6228 - categorical_accuracy: 0.8488 - val_loss: 1.3347 - val_categorical_accuracy: 0.5328\n",
            "Epoch 47/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.6233 - categorical_accuracy: 0.8352 - val_loss: 1.4766 - val_categorical_accuracy: 0.5456\n",
            "Epoch 48/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.5565 - categorical_accuracy: 0.8516 - val_loss: 1.5041 - val_categorical_accuracy: 0.5296\n",
            "Epoch 49/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.5217 - categorical_accuracy: 0.8636 - val_loss: 1.8660 - val_categorical_accuracy: 0.4576\n",
            "Epoch 50/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.5265 - categorical_accuracy: 0.8352 - val_loss: 1.6555 - val_categorical_accuracy: 0.5576\n",
            "Epoch 51/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.4890 - categorical_accuracy: 0.8580 - val_loss: 1.5134 - val_categorical_accuracy: 0.5496\n",
            "Epoch 52/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.4749 - categorical_accuracy: 0.8448 - val_loss: 1.3900 - val_categorical_accuracy: 0.5216\n",
            "Epoch 53/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.4688 - categorical_accuracy: 0.8384 - val_loss: 1.5179 - val_categorical_accuracy: 0.5320\n",
            "Epoch 54/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.3951 - categorical_accuracy: 0.8744 - val_loss: 1.6808 - val_categorical_accuracy: 0.5312\n",
            "Epoch 55/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.3708 - categorical_accuracy: 0.8692 - val_loss: 2.1670 - val_categorical_accuracy: 0.4872\n",
            "Epoch 56/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.3748 - categorical_accuracy: 0.8568 - val_loss: 2.0234 - val_categorical_accuracy: 0.5248\n",
            "Epoch 57/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.3577 - categorical_accuracy: 0.8580 - val_loss: 1.8290 - val_categorical_accuracy: 0.5456\n",
            "Epoch 58/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.3403 - categorical_accuracy: 0.8548 - val_loss: 1.5913 - val_categorical_accuracy: 0.5384\n",
            "Epoch 59/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.3359 - categorical_accuracy: 0.8504 - val_loss: 1.4945 - val_categorical_accuracy: 0.5352\n",
            "Epoch 60/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.3025 - categorical_accuracy: 0.8684 - val_loss: 1.5674 - val_categorical_accuracy: 0.5144\n",
            "Epoch 61/75\n",
            "20/20 [==============================] - 1s 47ms/step - loss: 0.2707 - categorical_accuracy: 0.8708 - val_loss: 1.9936 - val_categorical_accuracy: 0.5240\n",
            "Epoch 62/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2796 - categorical_accuracy: 0.8540 - val_loss: 1.8706 - val_categorical_accuracy: 0.5344\n",
            "Epoch 63/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2378 - categorical_accuracy: 0.8684 - val_loss: 1.7630 - val_categorical_accuracy: 0.5640\n",
            "Epoch 64/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2266 - categorical_accuracy: 0.8676 - val_loss: 1.8136 - val_categorical_accuracy: 0.5480\n",
            "Epoch 65/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2440 - categorical_accuracy: 0.8568 - val_loss: 1.7650 - val_categorical_accuracy: 0.5280\n",
            "Epoch 66/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2235 - categorical_accuracy: 0.8716 - val_loss: 1.8502 - val_categorical_accuracy: 0.5408\n",
            "Epoch 67/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2345 - categorical_accuracy: 0.8732 - val_loss: 2.2375 - val_categorical_accuracy: 0.5344\n",
            "Epoch 68/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2461 - categorical_accuracy: 0.8628 - val_loss: 1.9277 - val_categorical_accuracy: 0.5088\n",
            "Epoch 69/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.3732 - categorical_accuracy: 0.8360 - val_loss: 1.2151 - val_categorical_accuracy: 0.5856\n",
            "Epoch 70/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.2622 - categorical_accuracy: 0.8616 - val_loss: 1.3226 - val_categorical_accuracy: 0.5376\n",
            "Epoch 71/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2298 - categorical_accuracy: 0.8672 - val_loss: 1.5968 - val_categorical_accuracy: 0.5152\n",
            "Epoch 72/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2063 - categorical_accuracy: 0.8804 - val_loss: 1.4780 - val_categorical_accuracy: 0.5616\n",
            "Epoch 73/75\n",
            "20/20 [==============================] - 1s 46ms/step - loss: 0.2247 - categorical_accuracy: 0.8696 - val_loss: 1.6519 - val_categorical_accuracy: 0.5704\n",
            "Epoch 74/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2235 - categorical_accuracy: 0.8672 - val_loss: 2.0056 - val_categorical_accuracy: 0.4944\n",
            "Epoch 75/75\n",
            "20/20 [==============================] - 1s 45ms/step - loss: 0.2340 - categorical_accuracy: 0.8584 - val_loss: 1.7149 - val_categorical_accuracy: 0.5576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svDKv3iYGeqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLYwZTAVL6JH",
        "colab_type": "text"
      },
      "source": [
        "Now consider the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQfP5qylL6JH",
        "colab_type": "code",
        "outputId": "a10e283a-aa1c-4021-b490-cb0554e2d5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model = load_model(\"final_model.hdf5\")\n",
        "\n",
        "x = np.zeros((1,num_steps))\n",
        "x[0,:] = [vocabulary[\"this\"],vocabulary[\"is\"],vocabulary[\"an\"],vocabulary[\"easy\"],vocabulary[\"test\"]]\n",
        "print((model.predict_proba(x)))\n",
        "print((model.predict_classes(x)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.47148433 0.52851564]\n",
            "  [0.47734493 0.522655  ]\n",
            "  [0.57648987 0.42351013]\n",
            "  [0.9694578  0.03054217]\n",
            "  [0.9968754  0.00312455]]]\n",
            "[[1 1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QEDMl-KHQC_s"
      },
      "source": [
        "## Prediction Method Used:\n",
        "Since the model used in above step takes in num_steps (5) tokens as inputs at a time and gives the ironic probability of each token at a time, Each tweet is broken down into a 5 grams [5 word sequences] at a time and is fed to the model to get the probability of each word being ironic or not . This allows the word to be considered in different contexts to check whether the given tweet is ironic or not.\n",
        "\n",
        "   Though the model during training ignores batches which are less than num_steps [last 2 tokens in a 12 word sentence], the method that i have used considers all the tokens in a given tweet, so that there is no context information that is lost , and most of the hashtags are present in a tweet at the end.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For Example:\n",
        "['Tripped', 'over', 'my', 'own', 'feet', 'three', 'times', 'in', 'the', 'hall']\n",
        "\n",
        "For the above tweet: The strategy that i have used to check whether the tweet is ironic or not is by considering 5 word sequences at a time and multiply the probability of each set to check the ironic probability.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[['Tripped', 'over', 'my', 'own', 'feet',] x  [ 'over', 'my', 'own', 'feet', ''three'' ]x  [ 'my',  'own',  'feet', 'three',' times'] x  ['own', 'feet', 'three', 'times', 'in'] x [''feet', 'three', 'times', 'in', 'the'] x [ 'three', 'times', 'in', 'the', 'hall']\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV-XUqk1KdxW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBTJCUH2YXVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_keras(test_data):\n",
        "  '''\n",
        "  input: test_data [dataframe with tweets in the second index just like the train data]\n",
        "  output: classification outputs in a list\n",
        "  '''\n",
        "  current_idx = 0\n",
        "  predictions = []\n",
        "  for i in range(len(test_data)):\n",
        "    current_idx = 0\n",
        "    num_tokens = len(test_data[i][2])\n",
        "    #identify the nunmber of batches of num_steps can be formed\n",
        "    batch_size = num_tokens - num_steps +1 if (num_tokens - num_steps >0) else 1\n",
        "    #inistialize each batch with zeroes\n",
        "    x = np.zeros((batch_size,num_steps))\n",
        "    while True:\n",
        "      #identify the number of steps to be considered based on the number of tokens present in the tweet\n",
        "      step_size = num_steps if (num_tokens> num_steps) else num_tokens\n",
        "      if (num_tokens> num_steps):\n",
        "        x[current_idx, :] = [vocabulary[w] for w in test_data[i][2][current_idx:current_idx + num_steps]]\n",
        "      else:\n",
        "        x[current_idx, :num_tokens] = [vocabulary[w] for w in test_data[i][2][current_idx:current_idx + num_tokens]]        \n",
        "      current_idx +=1\n",
        "      if (current_idx==batch_size):\n",
        "        break\n",
        "    predict_sentence = model.predict_proba(x)\n",
        "    # multiply the ironic probabilites for each set\n",
        "    ironic_prob = np.prod([s[0] for c in predict_sentence for s in c])\n",
        "    # multiply the non ironic probabilites for each set\n",
        "    non_ironic_prob = np.prod([s[1] for c in predict_sentence for s in c])\n",
        "    predictions.append(1 if ironic_prob > non_ironic_prob else 0)\n",
        "  return predictions\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZu5FsFczlmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = predict_keras(test[:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgKC6ZDanJPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = [y[1] for y in test[:]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7aEfB_0nl0T",
        "colab_type": "code",
        "outputId": "102cf75f-7924-44b2-9735-9851cdf6d630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "f1score = f1_score(actual, predictions, average='micro')\n",
        "print(f1score)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5622188905547226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0lGYFcM2L5C",
        "colab_type": "code",
        "outputId": "93e4ea31-89bc-46c1-d88b-c116242a01b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy = accuracy_score(actual,predictions)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5622188905547226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xBk0yVGrYrD",
        "colab_type": "code",
        "outputId": "bcef9d2d-d03e-43d0-ac03-52ae7dce6289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm = confusion_matrix(actual, predictions)\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[525 151]\n",
            " [433 225]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yZucGZmL6JM",
        "colab_type": "text"
      },
      "source": [
        "## Improving the above model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jNWZfkYTHMz",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Steps\n",
        "\n",
        "\n",
        "1.   All the URL's (image links and other website links present in the tweets) which dont help in anyway in detecting irony in a tweet are removed\n",
        "2.   The users which are tagges in a tweet are all replaced with @user in a tweet as the username does not help in any way in identifying whether a tweet is ironic or not.\n",
        "3. The hashtags which i consider to be the main features in identifying whether a tweet is ironic or not, have been added back again to the tweet if they are in camel case format like #NewYearNewMe is split into #New #Year #New #Me, so that each token can be identified separately.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YltxFiV-WtNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load all the tweets preserving the case this time\n",
        "formatted_data = load_tweets(preserve_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7r1GkIz-ge9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "# preprocess the tweets according to the rules mentioned above\n",
        "def preprocess_sentences(sentences):\n",
        "  processed_sentences = []\n",
        "  hashtagCamelCase = re.compile(r'[A-Z]{2,}(?![a-z])|[A-Z][a-z]+')\n",
        "  for line in sentences:\n",
        "    #Replace URLs by <website>:\n",
        "    line = re.sub(r\"https?://.+\",\"\",line)\n",
        "    #Replace usernames by <user>:\n",
        "    line = re.sub(r\"@[^\\s]*\",\"@user\",line)\n",
        "    hashtags =re.findall(r'#\\w+', line)\n",
        "    for hashtag in hashtags:      \n",
        "      words = hashtagCamelCase.findall(hashtag)  \n",
        "      for var in words:\n",
        "        line += \" #\"+var\n",
        "    processed_sentences.append(str.lower(line))\n",
        "  return processed_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7E3Ooj95qvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences = formatted_data['Tweet text'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs3L_yBh5tgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = preprocess_sentences(sentences=sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHlqJQsdevqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get all the labels from the tweet\n",
        "y = formatted_data['Label'].values\n",
        "#get the test and train features and labels \n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLoBMHTYcmSC",
        "colab_type": "text"
      },
      "source": [
        "#Approach 1\n",
        "##Word Embeddings\n",
        "Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.\n",
        "\n",
        "\n",
        "The model used in Task 4 had an embedding layer learning embeddings based on the vocabulary of the dataset. Since the vector space can not be restricted just based on the limited voabulary of the given dataset,  to improve the  performance of the model, an alternative  precomputed embedding space GloVe  is used.\n",
        "###GloVe\n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
        "\n",
        "## Bidirectional LSTM\n",
        "Bidirectional LSTMs are an extension of traditional LSTMs that can improve model performance on sequence classification problems.\n",
        "In problems where all timesteps of the input sequence are available, Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UalGuPKK7B-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 35\n",
        "hidden_size =100\n",
        "#To tokenize the data into a format that can be used by the word embeddings.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "# since each tweet text is not of the same length the remaining length is padded\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "\n",
        "embedding_dim = 100\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SKY0ACW1MrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the glove embedding file and extract the files\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_utP_KI8iR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as  np\n",
        "'''\n",
        "Reference: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
        "Only those words which are present in the vocabulary are checked\n",
        "Each word in the file is followed by its vector as a stream of floats\n",
        "'''\n",
        "\n",
        "def get_weight_matrix(filepath, word_index):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    weights = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                weights[idx] = np.array(\n",
        "                    values[1:], dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cemY1K98jBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "glove_path = 'glove.6B.100d.txt'\n",
        "weight_matrix = get_weight_matrix(glove_path,tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Il4rj_t7B7",
        "colab_type": "code",
        "outputId": "1b71f3be-2659-40d1-8c22-6abeb5bba2ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "\n",
        "hidden_size = 100\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, input_length=35,trainable=True, weights=[weight_matrix]))\n",
        "model.add((LSTM(hidden_size, return_sequences=True, recurrent_dropout=0.2)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Bidirectional(LSTM(hidden_size, return_sequences=True, recurrent_dropout=0.3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 35, 100)           833000    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 35, 100)           80400     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 35, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 35, 200)           160800    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 35, 200)           0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 7000)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                70010     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 1,144,221\n",
            "Trainable params: 1,144,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxMQRZAfM1DP",
        "colab_type": "code",
        "outputId": "7bec7be2-0b8f-4023-fc78-14d80b7f5c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2091
        }
      },
      "source": [
        "model.fit(X_train, y_train,epochs=60,verbose=True,validation_data=(X_test, y_test),batch_size=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2875 samples, validate on 959 samples\n",
            "Epoch 1/60\n",
            "2875/2875 [==============================] - 21s 7ms/step - loss: 0.6941 - acc: 0.5294 - val_loss: 0.6870 - val_acc: 0.5308\n",
            "Epoch 2/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.6721 - acc: 0.5718 - val_loss: 0.6783 - val_acc: 0.5401\n",
            "Epoch 3/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.6199 - acc: 0.6543 - val_loss: 0.6532 - val_acc: 0.6184\n",
            "Epoch 4/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.5110 - acc: 0.7391 - val_loss: 0.6954 - val_acc: 0.6163\n",
            "Epoch 5/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.3974 - acc: 0.8167 - val_loss: 0.7484 - val_acc: 0.6309\n",
            "Epoch 6/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.2348 - acc: 0.9057 - val_loss: 1.0655 - val_acc: 0.6017\n",
            "Epoch 7/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.1222 - acc: 0.9523 - val_loss: 1.3194 - val_acc: 0.6173\n",
            "Epoch 8/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0839 - acc: 0.9729 - val_loss: 1.6880 - val_acc: 0.6173\n",
            "Epoch 9/60\n",
            "2875/2875 [==============================] - 16s 6ms/step - loss: 0.0451 - acc: 0.9847 - val_loss: 2.1573 - val_acc: 0.6006\n",
            "Epoch 10/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0200 - acc: 0.9944 - val_loss: 2.5319 - val_acc: 0.6111\n",
            "Epoch 11/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0160 - acc: 0.9944 - val_loss: 2.7657 - val_acc: 0.6184\n",
            "Epoch 12/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0116 - acc: 0.9972 - val_loss: 2.9265 - val_acc: 0.6027\n",
            "Epoch 13/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0144 - acc: 0.9944 - val_loss: 3.1180 - val_acc: 0.6058\n",
            "Epoch 14/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0267 - acc: 0.9892 - val_loss: 2.8031 - val_acc: 0.6184\n",
            "Epoch 15/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0191 - acc: 0.9944 - val_loss: 2.8684 - val_acc: 0.6038\n",
            "Epoch 16/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0154 - acc: 0.9934 - val_loss: 3.3081 - val_acc: 0.6069\n",
            "Epoch 17/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0200 - acc: 0.9955 - val_loss: 2.9002 - val_acc: 0.6017\n",
            "Epoch 18/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0087 - acc: 0.9976 - val_loss: 3.3094 - val_acc: 0.6173\n",
            "Epoch 19/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0100 - acc: 0.9962 - val_loss: 3.2527 - val_acc: 0.5829\n",
            "Epoch 20/60\n",
            "2875/2875 [==============================] - 16s 6ms/step - loss: 0.0064 - acc: 0.9983 - val_loss: 3.6153 - val_acc: 0.5985\n",
            "Epoch 21/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0060 - acc: 0.9983 - val_loss: 3.7570 - val_acc: 0.6017\n",
            "Epoch 22/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 3.8058 - val_acc: 0.5985\n",
            "Epoch 23/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0060 - acc: 0.9979 - val_loss: 3.8160 - val_acc: 0.6079\n",
            "Epoch 24/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0064 - acc: 0.9979 - val_loss: 3.7847 - val_acc: 0.6038\n",
            "Epoch 25/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0056 - acc: 0.9983 - val_loss: 3.8400 - val_acc: 0.6058\n",
            "Epoch 26/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0117 - acc: 0.9965 - val_loss: 4.2192 - val_acc: 0.6152\n",
            "Epoch 27/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0363 - acc: 0.9878 - val_loss: 3.0634 - val_acc: 0.5860\n",
            "Epoch 28/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0107 - acc: 0.9972 - val_loss: 3.1745 - val_acc: 0.5892\n",
            "Epoch 29/60\n",
            "2875/2875 [==============================] - 16s 6ms/step - loss: 0.0135 - acc: 0.9958 - val_loss: 3.1929 - val_acc: 0.5944\n",
            "Epoch 30/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0058 - acc: 0.9983 - val_loss: 3.6239 - val_acc: 0.6100\n",
            "Epoch 31/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0061 - acc: 0.9979 - val_loss: 3.5145 - val_acc: 0.6090\n",
            "Epoch 32/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0070 - acc: 0.9983 - val_loss: 3.1910 - val_acc: 0.6100\n",
            "Epoch 33/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0076 - acc: 0.9965 - val_loss: 3.7657 - val_acc: 0.5933\n",
            "Epoch 34/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0072 - acc: 0.9972 - val_loss: 3.5131 - val_acc: 0.5985\n",
            "Epoch 35/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 3.6562 - val_acc: 0.6017\n",
            "Epoch 36/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0051 - acc: 0.9983 - val_loss: 3.7020 - val_acc: 0.6006\n",
            "Epoch 37/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0051 - acc: 0.9986 - val_loss: 3.8069 - val_acc: 0.6027\n",
            "Epoch 38/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0117 - acc: 0.9965 - val_loss: 3.6042 - val_acc: 0.5693\n",
            "Epoch 39/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0160 - acc: 0.9934 - val_loss: 3.7286 - val_acc: 0.5819\n",
            "Epoch 40/60\n",
            "2875/2875 [==============================] - 16s 6ms/step - loss: 0.0143 - acc: 0.9955 - val_loss: 3.5648 - val_acc: 0.5829\n",
            "Epoch 41/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0096 - acc: 0.9969 - val_loss: 3.8344 - val_acc: 0.6017\n",
            "Epoch 42/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0073 - acc: 0.9976 - val_loss: 4.0860 - val_acc: 0.6048\n",
            "Epoch 43/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 4.1977 - val_acc: 0.5892\n",
            "Epoch 44/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0069 - acc: 0.9983 - val_loss: 3.9905 - val_acc: 0.5787\n",
            "Epoch 45/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 4.1193 - val_acc: 0.5923\n",
            "Epoch 46/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0119 - acc: 0.9962 - val_loss: 3.5754 - val_acc: 0.5912\n",
            "Epoch 47/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0079 - acc: 0.9972 - val_loss: 4.0578 - val_acc: 0.5860\n",
            "Epoch 48/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0205 - acc: 0.9934 - val_loss: 3.6428 - val_acc: 0.5860\n",
            "Epoch 49/60\n",
            "2875/2875 [==============================] - 16s 6ms/step - loss: 0.0090 - acc: 0.9979 - val_loss: 4.0921 - val_acc: 0.5881\n",
            "Epoch 50/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0054 - acc: 0.9983 - val_loss: 4.2758 - val_acc: 0.5912\n",
            "Epoch 51/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0069 - acc: 0.9969 - val_loss: 4.0191 - val_acc: 0.5892\n",
            "Epoch 52/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0110 - acc: 0.9955 - val_loss: 3.7848 - val_acc: 0.5871\n",
            "Epoch 53/60\n",
            "2875/2875 [==============================] - 16s 5ms/step - loss: 0.0154 - acc: 0.9955 - val_loss: 3.6820 - val_acc: 0.5860\n",
            "Epoch 54/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0117 - acc: 0.9955 - val_loss: 3.5756 - val_acc: 0.5985\n",
            "Epoch 55/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0060 - acc: 0.9979 - val_loss: 3.8284 - val_acc: 0.5881\n",
            "Epoch 56/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0062 - acc: 0.9983 - val_loss: 4.0277 - val_acc: 0.5839\n",
            "Epoch 57/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0059 - acc: 0.9979 - val_loss: 3.9943 - val_acc: 0.5871\n",
            "Epoch 58/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 3.7180 - val_acc: 0.5819\n",
            "Epoch 59/60\n",
            "2875/2875 [==============================] - 15s 5ms/step - loss: 0.0082 - acc: 0.9979 - val_loss: 4.1646 - val_acc: 0.5819\n",
            "Epoch 60/60\n",
            "2875/2875 [==============================] - 16s 6ms/step - loss: 0.0060 - acc: 0.9983 - val_loss: 4.2482 - val_acc: 0.5892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9a90fdb7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TijqlkMREFhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = model.predict_classes(X_test)\n",
        "predicted = predicted.reshape(len(predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3DAvM9gsqm9",
        "colab_type": "code",
        "outputId": "7bf6591c-cefb-4949-f304-b974a4d9b0f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "accuracy = accuracy_score(y_test, predicted)\n",
        "print(\"Accuracy of the model is \" +str(accuracy))\n",
        "cm = confusion_matrix(y_test, predicted,labels=[0, 1])\n",
        "print(\"Confusion Matrix\")\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model is 0.5891553701772679\n",
            "Confusion Matrix\n",
            "[[294 174]\n",
            " [220 271]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbI8OeWJiCHf",
        "colab_type": "code",
        "outputId": "6137e141-71e2-4ae9-c46d-864e89cbcd86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1score = f1_score(y_test, predicted, average='micro')\n",
        "print(\"F1 score  of the model is \" +str(f1score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score  of the model is 0.5891553701772679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L2COatoi4Ie",
        "colab_type": "text"
      },
      "source": [
        "# Approach 2\n",
        "\n",
        "##Tf-Idf\n",
        "30% of the tokens present in the dataset are not present in the GLove because the way in which the tweets are normally present. With a lot of misspelt words and slang languages. Another approach to vectorize each sentence is by using Tf-Idf approach. Where Tf represents the term frequency of each token, and Idf stands for Inverse document Frequency which is used to penalize words which appear across multiple tweets [Documents] and helps very little in distinguishing one document from another.\n",
        "\n",
        "Both Unigrams and Bigrams of words in the tweet are considered in training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCJ6preaVnOH",
        "colab_type": "code",
        "outputId": "524d6d10-42b4-41f7-cc2c-596ec1cdd474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2108
        }
      },
      "source": [
        "tweet_tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True).tokenize\n",
        "vectorizer = TfidfVectorizer(strip_accents=\"unicode\", analyzer=\"word\", tokenizer=tweet_tokenizer,ngram_range=(1,2))\n",
        "vectorizer.fit(sentences_train)\n",
        "\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test = vectorizer.transform(sentences_test)\n",
        "input_dim = X_train.shape[1]  \n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train,epochs=60,verbose=True,validation_data=(X_test, y_test),batch_size=30)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "predicted = model.predict_classes(X_test)\n",
        "predicted = predicted.reshape(len(predicted))\n",
        "f1score = f1_score(y_test, predicted, average='micro')\n",
        "print(\"F1 score  of the model is \" +str(f1score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2875 samples, validate on 959 samples\n",
            "Epoch 1/60\n",
            "2875/2875 [==============================] - 3s 1ms/step - loss: 0.6909 - acc: 0.5590 - val_loss: 0.6855 - val_acc: 0.6330\n",
            "Epoch 2/60\n",
            "2875/2875 [==============================] - 2s 731us/step - loss: 0.6480 - acc: 0.8661 - val_loss: 0.6676 - val_acc: 0.6340\n",
            "Epoch 3/60\n",
            "2875/2875 [==============================] - 2s 739us/step - loss: 0.5517 - acc: 0.9527 - val_loss: 0.6463 - val_acc: 0.6184\n",
            "Epoch 4/60\n",
            "2875/2875 [==============================] - 2s 738us/step - loss: 0.4398 - acc: 0.9690 - val_loss: 0.6315 - val_acc: 0.6236\n",
            "Epoch 5/60\n",
            "2875/2875 [==============================] - 2s 741us/step - loss: 0.3287 - acc: 0.9823 - val_loss: 0.6246 - val_acc: 0.6309\n",
            "Epoch 6/60\n",
            "2875/2875 [==============================] - 2s 736us/step - loss: 0.2497 - acc: 0.9823 - val_loss: 0.6242 - val_acc: 0.6277\n",
            "Epoch 7/60\n",
            "2875/2875 [==============================] - 2s 731us/step - loss: 0.1849 - acc: 0.9903 - val_loss: 0.6279 - val_acc: 0.6298\n",
            "Epoch 8/60\n",
            "2875/2875 [==============================] - 2s 743us/step - loss: 0.1452 - acc: 0.9934 - val_loss: 0.6338 - val_acc: 0.6361\n",
            "Epoch 9/60\n",
            "2875/2875 [==============================] - 2s 741us/step - loss: 0.1122 - acc: 0.9930 - val_loss: 0.6423 - val_acc: 0.6277\n",
            "Epoch 10/60\n",
            "2875/2875 [==============================] - 2s 731us/step - loss: 0.0908 - acc: 0.9941 - val_loss: 0.6515 - val_acc: 0.6267\n",
            "Epoch 11/60\n",
            "2875/2875 [==============================] - 2s 737us/step - loss: 0.0791 - acc: 0.9941 - val_loss: 0.6616 - val_acc: 0.6267\n",
            "Epoch 12/60\n",
            "2875/2875 [==============================] - 2s 745us/step - loss: 0.0648 - acc: 0.9951 - val_loss: 0.6706 - val_acc: 0.6309\n",
            "Epoch 13/60\n",
            "2875/2875 [==============================] - 2s 736us/step - loss: 0.0544 - acc: 0.9955 - val_loss: 0.6812 - val_acc: 0.6257\n",
            "Epoch 14/60\n",
            "2875/2875 [==============================] - 2s 731us/step - loss: 0.0479 - acc: 0.9955 - val_loss: 0.6920 - val_acc: 0.6257\n",
            "Epoch 15/60\n",
            "2875/2875 [==============================] - 2s 737us/step - loss: 0.0439 - acc: 0.9948 - val_loss: 0.7015 - val_acc: 0.6277\n",
            "Epoch 16/60\n",
            "2875/2875 [==============================] - 2s 736us/step - loss: 0.0390 - acc: 0.9958 - val_loss: 0.7106 - val_acc: 0.6246\n",
            "Epoch 17/60\n",
            "2875/2875 [==============================] - 2s 739us/step - loss: 0.0333 - acc: 0.9972 - val_loss: 0.7205 - val_acc: 0.6236\n",
            "Epoch 18/60\n",
            "2875/2875 [==============================] - 2s 732us/step - loss: 0.0309 - acc: 0.9962 - val_loss: 0.7300 - val_acc: 0.6215\n",
            "Epoch 19/60\n",
            "2875/2875 [==============================] - 2s 745us/step - loss: 0.0284 - acc: 0.9958 - val_loss: 0.7392 - val_acc: 0.6225\n",
            "Epoch 20/60\n",
            "2875/2875 [==============================] - 2s 732us/step - loss: 0.0259 - acc: 0.9972 - val_loss: 0.7497 - val_acc: 0.6184\n",
            "Epoch 21/60\n",
            "2875/2875 [==============================] - 2s 742us/step - loss: 0.0233 - acc: 0.9965 - val_loss: 0.7589 - val_acc: 0.6204\n",
            "Epoch 22/60\n",
            "2875/2875 [==============================] - 2s 734us/step - loss: 0.0229 - acc: 0.9965 - val_loss: 0.7685 - val_acc: 0.6215\n",
            "Epoch 23/60\n",
            "2875/2875 [==============================] - 2s 737us/step - loss: 0.0200 - acc: 0.9972 - val_loss: 0.7778 - val_acc: 0.6225\n",
            "Epoch 24/60\n",
            "2875/2875 [==============================] - 2s 742us/step - loss: 0.0187 - acc: 0.9969 - val_loss: 0.7863 - val_acc: 0.6204\n",
            "Epoch 25/60\n",
            "2875/2875 [==============================] - 2s 737us/step - loss: 0.0184 - acc: 0.9972 - val_loss: 0.7960 - val_acc: 0.6204\n",
            "Epoch 26/60\n",
            "2875/2875 [==============================] - 2s 741us/step - loss: 0.0183 - acc: 0.9979 - val_loss: 0.8052 - val_acc: 0.6184\n",
            "Epoch 27/60\n",
            "2875/2875 [==============================] - 2s 735us/step - loss: 0.0154 - acc: 0.9979 - val_loss: 0.8141 - val_acc: 0.6204\n",
            "Epoch 28/60\n",
            "2875/2875 [==============================] - 2s 736us/step - loss: 0.0148 - acc: 0.9976 - val_loss: 0.8230 - val_acc: 0.6194\n",
            "Epoch 29/60\n",
            "2875/2875 [==============================] - 2s 740us/step - loss: 0.0150 - acc: 0.9979 - val_loss: 0.8312 - val_acc: 0.6184\n",
            "Epoch 30/60\n",
            "2875/2875 [==============================] - 2s 743us/step - loss: 0.0139 - acc: 0.9983 - val_loss: 0.8405 - val_acc: 0.6204\n",
            "Epoch 31/60\n",
            "2875/2875 [==============================] - 2s 738us/step - loss: 0.0142 - acc: 0.9979 - val_loss: 0.8498 - val_acc: 0.6225\n",
            "Epoch 32/60\n",
            "2875/2875 [==============================] - 2s 740us/step - loss: 0.0118 - acc: 0.9983 - val_loss: 0.8582 - val_acc: 0.6236\n",
            "Epoch 33/60\n",
            "2875/2875 [==============================] - 2s 740us/step - loss: 0.0129 - acc: 0.9972 - val_loss: 0.8666 - val_acc: 0.6215\n",
            "Epoch 34/60\n",
            "2875/2875 [==============================] - 2s 735us/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.8744 - val_acc: 0.6184\n",
            "Epoch 35/60\n",
            "2875/2875 [==============================] - 2s 730us/step - loss: 0.0107 - acc: 0.9972 - val_loss: 0.8823 - val_acc: 0.6236\n",
            "Epoch 36/60\n",
            "2875/2875 [==============================] - 2s 733us/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.8896 - val_acc: 0.6246\n",
            "Epoch 37/60\n",
            "2875/2875 [==============================] - 2s 732us/step - loss: 0.0097 - acc: 0.9983 - val_loss: 0.8993 - val_acc: 0.6267\n",
            "Epoch 38/60\n",
            "2875/2875 [==============================] - 2s 736us/step - loss: 0.0105 - acc: 0.9979 - val_loss: 0.9055 - val_acc: 0.6267\n",
            "Epoch 39/60\n",
            "2875/2875 [==============================] - 2s 803us/step - loss: 0.0115 - acc: 0.9983 - val_loss: 0.9141 - val_acc: 0.6267\n",
            "Epoch 40/60\n",
            "2875/2875 [==============================] - 2s 817us/step - loss: 0.0101 - acc: 0.9979 - val_loss: 0.9243 - val_acc: 0.6246\n",
            "Epoch 41/60\n",
            "2875/2875 [==============================] - 2s 788us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.9304 - val_acc: 0.6277\n",
            "Epoch 42/60\n",
            "2875/2875 [==============================] - 2s 735us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.9380 - val_acc: 0.6267\n",
            "Epoch 43/60\n",
            "2875/2875 [==============================] - 2s 743us/step - loss: 0.0094 - acc: 0.9979 - val_loss: 0.9480 - val_acc: 0.6267\n",
            "Epoch 44/60\n",
            "2875/2875 [==============================] - 2s 735us/step - loss: 0.0083 - acc: 0.9979 - val_loss: 0.9565 - val_acc: 0.6257\n",
            "Epoch 45/60\n",
            "2875/2875 [==============================] - 2s 738us/step - loss: 0.0094 - acc: 0.9972 - val_loss: 0.9619 - val_acc: 0.6246\n",
            "Epoch 46/60\n",
            "2875/2875 [==============================] - 2s 737us/step - loss: 0.0079 - acc: 0.9983 - val_loss: 0.9707 - val_acc: 0.6204\n",
            "Epoch 47/60\n",
            "2875/2875 [==============================] - 2s 739us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.9756 - val_acc: 0.6246\n",
            "Epoch 48/60\n",
            "2875/2875 [==============================] - 2s 739us/step - loss: 0.0080 - acc: 0.9979 - val_loss: 0.9821 - val_acc: 0.6236\n",
            "Epoch 49/60\n",
            "2875/2875 [==============================] - 2s 732us/step - loss: 0.0086 - acc: 0.9986 - val_loss: 0.9888 - val_acc: 0.6236\n",
            "Epoch 50/60\n",
            "2875/2875 [==============================] - 2s 739us/step - loss: 0.0077 - acc: 0.9983 - val_loss: 0.9993 - val_acc: 0.6246\n",
            "Epoch 51/60\n",
            "2875/2875 [==============================] - 2s 739us/step - loss: 0.0082 - acc: 0.9979 - val_loss: 1.0057 - val_acc: 0.6267\n",
            "Epoch 52/60\n",
            "2875/2875 [==============================] - 2s 726us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 1.0145 - val_acc: 0.6246\n",
            "Epoch 53/60\n",
            "2875/2875 [==============================] - 2s 732us/step - loss: 0.0082 - acc: 0.9983 - val_loss: 1.0235 - val_acc: 0.6225\n",
            "Epoch 54/60\n",
            "2875/2875 [==============================] - 2s 737us/step - loss: 0.0071 - acc: 0.9979 - val_loss: 1.0303 - val_acc: 0.6236\n",
            "Epoch 55/60\n",
            "2875/2875 [==============================] - 2s 733us/step - loss: 0.0059 - acc: 0.9986 - val_loss: 1.0414 - val_acc: 0.6215\n",
            "Epoch 56/60\n",
            "2875/2875 [==============================] - 2s 743us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 1.0464 - val_acc: 0.6225\n",
            "Epoch 57/60\n",
            "2875/2875 [==============================] - 2s 738us/step - loss: 0.0062 - acc: 0.9983 - val_loss: 1.0580 - val_acc: 0.6236\n",
            "Epoch 58/60\n",
            "2875/2875 [==============================] - 2s 736us/step - loss: 0.0075 - acc: 0.9976 - val_loss: 1.0632 - val_acc: 0.6246\n",
            "Epoch 59/60\n",
            "2875/2875 [==============================] - 2s 736us/step - loss: 0.0085 - acc: 0.9972 - val_loss: 1.0650 - val_acc: 0.6225\n",
            "Epoch 60/60\n",
            "2875/2875 [==============================] - 2s 741us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 1.0718 - val_acc: 0.6257\n",
            "Testing Accuracy:  0.6257\n",
            "F1 score  of the model is 0.6256517205422315\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}